import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import scipy.stats as st
from sklearn import ensemble, tree, linear_model
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler, Normalizer
import os
import plotly.express as px

path = "/content/drive/MyDrive/Colab Notebooks/New/SVM"
os.chdir(path)

dataset = pd.read_csv("D_W_Ave.csv")
dataset = dataset.drop(columns = "Case #")
dataset.head()
print(dataset.shape)

dataset.plot(kind = 'scatter', x = 'Width', y = 'Depth', figsize = (20, 14),
             s = dataset['Fo']*200, label = 'Fo',
             c = dataset ['Pr'], cmap = plt.get_cmap('jet'))
correlation = dataset.corr()
print (correlation ["Depth"].sort_values(ascending = False), '\n')
cm = np.corrcoef(dataset)
k = 15
cols = correlation.nlargest(k, 'DepthOverHalfWidth')['DepthOverHalfWidth'].index
print(cols)
cm = np.corrcoef(dataset[cols].values.T)
f, ax = plt.subplots(figsize = (12, 14))
sns.heatmap(cm, vmax = 0.8, linewidths = 0.01, square = True, annot = True, cmap = 'viridis',
            linecolor = "white", xticklabels = cols.values, annot_kws = {'size' : 12}, yticklabels = cols.values)
X_labels = dataset [["Fo", "Pr", "Ste", "Ma"]]
Y_label = dataset.Depth
print(X_labels.shape)
print(Y_label.shape)
X_labels
label = "Depth"
px.scatter_matrix(dataset, dataset, color=label, \
                  width=len(dataset) * 20, 
                  height=len(dataset) * 20)
                  for feature in dataset:
  fig = px.histogram(dataset, x=feature, marginal="box")
  fig.show() 
  ## 3 ta figure Wdith va Depth va ratio be dard mikhore
  from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor

from sklearn.linear_model import LinearRegression

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.metrics import mean_squared_error
import xgboost as xgb
from lightgbm import LGBMRegressor
dataset = dataset.drop(['Width', 'DepthOverHalfWidth'], axis = 1)
train, test = train_test_split(dataset, test_size=0.1, random_state=42)
train, val = train_test_split(train, test_size=0.2, random_state=42)
X_train = train.drop('Depth', axis = 1)
Y_train = train['Depth']
X_val = val.drop('Depth', axis = 1)
Y_val = val['Depth']
X_test = test.drop('Depth', axis = 1)
Y_test = test['Depth']
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_train = pd.DataFrame(X_train_scaled)
X_val_scaled = scaler.fit_transform(X_val)
X_val = pd.DataFrame(X_val_scaled)
X_test_scaled = scaler.fit_transform(X_test)
X_test = pd.DataFrame(X_test_scaled)
print('X__train_scaled: ', X_train.shape)
print('Y_train: ', Y_train.shape)
print('X_val_scaled:', X_val_scaled.shape)
print('Y_val:', Y_val.shape)
print('X_test_scaled: ', X_test.shape)
print('Y_test: ', Y_test.shape)
models = [KNeighborsRegressor(), DecisionTreeRegressor(), RandomForestRegressor(), SVR(), GradientBoostingRegressor(), LinearRegression(), LGBMRegressor()]
for m in models:
  m.fit(X_train_scaled, Y_train)
  Y_pred = m.predict(X_train_scaled)
  RMSE = np.sqrt(mean_squared_error(Y_train, Y_pred))
  print (f'model: {str(m)}', RMSE)
  
  def getTotalAbsoluteError(X_t, y_t, modelInput):
  predictValues = modelInput.predict(X_t)
  errors = y_t-predictValues
  return (sum(abs(errors)))
  err = []
err_RMSE=[]

errIndex = []
errTest = []
for treedepth in range (1,12):
  Selected_model = DecisionTreeRegressor(random_state=241, max_depth = treedepth)
  model12 = Selected_model.fit(X_train_scaled, Y_train)
  err.append(getTotalAbsoluteError( X_train_scaled, Y_train, model12)) # Calculating MAE for each tree depth training set
  errTest.append(getTotalAbsoluteError( X_val_scaled, Y_val, model12)) # Calculating MAE for each treedepth validation set
  Y_pred = model12.predict(X_train_scaled)
  err_RMSE.append(np.sqrt(mean_squared_error(Y_train, Y_pred))) ## Calculating RMSE error for each treedepth
  errIndex.append(treedepth)
fig, ax = plt.subplots()
ax.plot(errIndex, err, 'rs-', color = "orange", linewidth = 1.5)
ax.plot(errIndex, errTest, 'rs-', color = "red", linewidth = 1.5)
ax.set_xlabel("max Tree Depth")
ax.set_ylabel("Total Error (MAE)")
plt.legend(["Train", "Validate"])
fig2, ax2 = plt.subplots( figsize = (7, 5))
ax2.plot(errIndex, err_RMSE, 'rs-', color = "blue", linewidth = 1.5)
ax2.set_xlabel("max Tree Depth")
ax2.set_ylabel("Total Error (RMSE)")
plt.legend(["Train", "Validate"])
### Checking KNeighborRegressor k values
rmse_val = []
for k in range (35):
  k+=1
  knn = KNeighborsRegressor(n_neighbors = k)
  knn.fit(X_train_scaled, Y_train)
  Y_pred = knn.predict(X_train_scaled)
  RMSE = np.sqrt(mean_squared_error(Y_train, Y_pred))
  rmse_val.append(RMSE)
  print ('RMSE value for k = ', k, 'is ', RMSE)
for i in range(len(Y_pred)):
  if Y_pred[i]!= Y_train.iloc[i]:
    print("Y_pred is: ", Y_pred[i], "Y_test is: ", Y_train.iloc[i],"i is: ", i)
